In this discussion, we will cover the topic of propensity score and propensity space based methods for average treatment effect estimation. The propensity score is a crucial component in observational studies and causal inference. It represents the probability of receiving treatment in a randomized trial, given a set of covariates. In a randomized trial, the propensity score remains constant, such as in a 50-50 randomized trial where the score is uniformly 0.5. Analyzing the spread of propensity scores in an observational study can provide insight into how far it deviates from a randomized trial. 

One interesting aspect of the propensity score is its connection to machine learning and recent discussions on using machine learning approaches in propensity-based methods. Propensity score-based ideas will play a key role in developing best practice methods for average treatment effect estimation using machine learning. 

The inverse propensity weighted transformation is a significant concept related to propensity scores. Under the condition of uncompoundedness, the average treatment effect can be characterized using the inverse propensity weighted transformation. This representation of the average treatment effect is beneficial as it only depends on actual outcomes, which are measurable, rather than potential outcomes, which cannot be directly observed. 

Based on this representation, a practical estimator for the average treatment effect, known as the inverse propensity weighted estimator, can be derived. This estimator is unbiased, meaning it provides an accurate estimate of the average treatment effect. Similar ideas can be found in other areas of statistics, referred to as importance weighting or harvest Thompson sampling. The unbiasedness of the inverse propensity weighted estimator can be explained by considering that treatment assignment is binary, allowing for the replacement of potential outcomes with actual outcomes in the estimation process.
In this section, the researcher discusses the use of propensity scores in estimating the average treatment effect. They explain that the ratios in the formula for the estimator cancel out on expectation, resulting in unbiasedness. They mention that this relies on the assumption of unconfoundedness. They introduce the concept of the Oracle IPW estimator, which uses the true propensity scores. They then propose using estimated propensity scores instead, obtained through machine learning methods, and discuss the feasibility of this approach. They suggest that further analysis is needed to determine the performance of the feasible IPW estimator with estimated propensity scores.
In this discussion, we examine the feasibility of using the Oracle IPW estimator by comparing it to the feasible IPW estimator. We start by understanding the properties we desire in the feasible estimator and studying the gap between the two estimators. We assume the data points are independent and identically distributed, and with this assumption, we can see that the Oracle IPW estimator is an average of unbiased estimators. Therefore, the Central Limit Theorem applies and justifies the construction of confidence intervals for the average treatment effect.

We hope that the feasible estimator with estimated propensity scores behaves similarly to the Oracle estimator and converges at a similar rate. For this to happen, we want the difference between the feasible and Oracle IPW estimators to be small relative to the stochastic fluctuations in the IPW estimator. If this is the case, we can ignore the errors due to propensity score estimation when studying the estimator. To confirm this, we aim to bound the difference between the two estimators using the Cauchy-Schwarz inequality. By doing so, we can analyze whether the contribution of propensity score estimation can be ignored.
In this discussion, the focus is on the discrepancy between the estimated propensity scores used in inverse probability weighting (IPW) and the true propensity scores. It is found that the difference between these scores is on the order of the root mean squared error of the estimated propensity scores. This discrepancy is shown to have an upper bound, which depends on the root mean squared error. However, it is concluded that this bound is not strong enough because the root mean squared error in machine learning methods is generally much larger than the required threshold. It is further explained that IPW with machine learning-based estimated propensity scores tends to have larger errors compared to using true propensity scores, leading to less reliable results.
The main problem with using machine learning methods in estimation is that although they can provide accurate results in large samples, their estimation error is significant in finite samples. This error needs to be taken into account when using machine learning for average treatment effect estimation, as ignoring it will lead to inaccurate estimators and confidence intervals. Confidence intervals built using traditional approaches, such as inverse probability weighting (IPW), become useless when machine learning is used to estimate propensity scores due to the introduction of a larger error term. However, there are approaches that can address this issue and still make use of machine learning methods. These approaches involve carefully selecting the estimator and considering the error of the machine learning-based predictions. Simple estimators can be used in combination with machine learning-based predictions, as long as strict attention is paid to the selection and properties of the estimator. Regression adjustments and IPW alone are insufficient in this case. The lecture will introduce an estimator that resolves these challenges.
